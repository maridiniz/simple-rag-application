# Aplica√ß√£o RAG Simples
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![LangChain](https://img.shields.io/badge/LangChain-4CAF50?style=flat-square&logo=langchain&logoColor=black)](https://python.langchain.com)
[![LangGraph](https://img.shields.io/badge/LangGraph-9E9E9E?style=flat-square&logo=python&logoColor=white)](https://langchain-ai.github.io/langgraph/)
[![LangSmith](https://img.shields.io/badge/LangSmith-Enabled-blue?logo=langchain&logoColor=white)](https://smith.langchain.com/)

---

# Objetivo do projeto
O projeto tem o intuito apenas de demonstrar o passo a passo de uma simples aplica√ß√£o RAG (Retrieval-Augmented Generation). S√£o utilizados os componentes dos frameworks LangChain, langGraph e alguns componentes integrados como Google GenAI para os passos de vetoriza√ß√£o e gera√ß√£o do output, assim como tamb√©m o Chroma client para indexa√ß√£o dos embeddings localmente (no caso deste projeto) e posteriormente o passo de recupera√ß√£o. Tamb√©m √© poss√≠vel utilizar ainda em conjunto outros frameworks para observabilidade da aplica√ß√£o, como por exemplo o LangSmith, onde √© poss√≠vel monitorar, realizar debugging e etc. Existem alguns exemplos de como configurar essa etapa, caso tenha interesse, veja a [documenta√ß√£o oficial](https://docs.smith.langchain.com/observability). √â poss√≠vel encontrar todas as informa√ß√µes sobre todos os passos sobre RAG entre outras funcionalidades diretamenta na documenta√ß√£o oficial do [LangChain](https://python.langchain.com/docs/introduction/). Quanto √† documenta√ß√£o oficial do [LangGraph](https://langchain-ai.github.io/langgraph/), tamb√©m cont√©m todas as informa√ß√µes necess√°rias.

O RAG √© uma aplica√ß√£o no modelo de perguntas e respostas, onde √© realizada uma busca nos nossos pr√≥prios arquivos, ao final, tanto a pergunta quanto o conte√∫do recuperado (resposta) s√£o passados ao modelo de LLM escolhido para gerar um output mais amig√°vel ao usu√°rio. 

---

# Configura√ß√£o do Ambiente

## Pr√©-requisitos:

- `langchain-google-genai` ou `langchain_openai` entre outros [modelos](https://python.langchain.com/docs/integrations/text_embedding/) que desejar.
- `langchain-chroma` dentre v√°rios outros clientes [dispon√≠veis](https://python.langchain.com/docs/integrations/vectorstores/).
- `langchain-community 0.3+`
- `langchain-text-splitters 0.3+`
- `langchain-core 0.3+`
- `langchain 0.3+`
- `langsmith 0.4+`
- `langgraph 0.5+`
- `python-dotenv 1.1+`

## Pr√©-requisitos nativos:

- `os`
- `typing_extensions`

## Instala√ß√£o

1. Instale as depend√™ncias:

```python
pip install langchain-google-genai langchain-chroma langchain-community langchain langsmith langgraph python-dotenv
```

2. Clone este reposit√≥rio:

```bash
git clone git@github.com:maridiniz/simple-rag-application.git
```

## Configura√ß√£o da chave de API

Para prosseguir com o projeto √© necess√°rio utilizar um LLM que far√° o processo de vetoriza√ß√£o, indexa√ß√£o e tamb√©m participar√° do processo de gerar o output para o usu√°rio, por tanto,  ser√° necess√°ria utilizar uma chave de API de algum modelo de LLM ou utilizar um modelo local. Para definir a chave de API do modelo LLM escolhido em uma vari√°vel de ambiente.

1. M√©todo powersehll (Persiste mesmo ap√≥s reiniciar o sistema):

```powershell
[System.Environment]::SetEnvironmentVariable('GOOGLE_API_KEY', 'insira aqui sua chave', 'User')
```

2. M√©todo CMD (Permance mesmo ap√≥s reiniciar):

```cmd
setx GOOGLE_API_KEY "insira-aqui-sua-chave"
```

Verifique se funcionou:

```powershell
echo $env:GOOGLE_API_KEY
```

```cmd
echo %GOOGLE_API_KEY%
```

Alternativa com o m√≥dulo getpass:

```python
import getpass
import os

if not os.environ.get("GOOGLE_API_KEY"):
  os.environ["GOOGLE_API_KEY"] = getpass.getpass("Insira sua chave de API aqui: ")
```

## Observabilidade

Para observabilidade da aplica√ß√£o, √© poss√≠vel integrar ao `LangSmith`, um framework que possibilita debbugings, monitora√ß√£o etc.

Temos algumas op√ß√µes de como estabelecer a observabilidade com LangSmith, na pr√≥pria [p√°gina oficial](https://docs.smith.langchain.com/observability) tamb√©m tem as informa√ß√µes:

1. Atrav√©s do arquivo com extens√£o .env:

```python
1 LANGSMITH_TRACING="true"
2 LANGSMITH_ENDPOINT="https://api.smith.langchain.com"
3 LANGSMITH_API_KEY="<your-api-key>"
4 LANGSMITH_PROJECT="pr-crazy-formula-87"
5 OPENAI_API_KEY="<your-openai-api-key>"
```

Esse arquivo pode ser colocado no mesmo diret√≥rio do script da aplica√ß√£o, caso haja apenas um script, mas se houver m√∫ltiplos scripts que utilizam este arquivo, pode ser colocado no diret√≥rio principal, ou mesmo specificar o diret√≥rio como argumento na fun√ß√£o `load_dotenv()`, [aqui](https://pypi.org/project/python-dotenv/) tem as informa√ß√µes oficiais do m√≥dulo `dotenv` e seus componentes.

```python
# Impotando depend√™ncia:
from dotenv import load_dotenv

# A fun√ß√£o carrega as vari√°veis contidas no arquivo `.env`:
load_dotenv()

# --- Restante da aplica√ß√£o ---
```

2. Atrav√©s do pr√≥prio script da aplica√ß√£o:

```python
import getpass
import os

os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = getpass.getpass()
```

3. Atrav√©s das vari√°veis de ambiente:

```powershell
[System.Environment]::SetEnvironmentVariable("LANGSMITH_TRACING", "true", "User")
[System.Environment]::SetEnvironmentVariable("LANGSMITH_API_KEY", "your_api_key_here", "User")
```
√â muito importante que a chave de API do modelo escolhido (ChatGpt, Gemini etc) estejam devidamente configuradas nas vari√°veis de ambiente com qualquer um dos m√©todos citados, para que n√°o ocorram erros.

---

# Estrutura da Aplica√ß√£o

A aplica√ß√£o segue cinco passos:

O primeiro passo √© uma fun√ß√£o que define toda a l√≥gica para o processamento dos arquivos que ser√£o futuramente vetorizados e indexados em um diret√≥rio local. Primeiro, os arquivos s√£o carregados do diret√≥rio onde se encontram, logo depois s√£o contidos em um √∫nico objeto como uma unidade de texto. Ao final, essa unidade de texto √© dividida em v√°rias partes menores para que sejam futuramente transformadas em embeddings.

![](/image/data_processing.png)

```python
# Definindo o diret√≥rio onde est√£o os arquivos pdf:
from langchain_community.document_loaders import PyPDFDirectoryLoader

directory = "../docs"  # Indicamos onde est√° o diret√≥rio com os arquivos desejamos processar.

# Instanciando a classe que usaremos para carregar nossos arquivos:
loaded = PyPDFDirectoryLoader(path=directory, glob="**/*.pdf")
docs = loaded.load() 
```
No exemplo acima, utilizamos o PyPDFDirectoryLoader, um dos componentes do m√≥dulo `document_loader` que √© espec√≠fico para carregar v√°rios arquivos pdf de um diret√≥rio, este m√≥dulo possui v√°rias op√ß√µes para carregar arquivos contidos em diferentes fontes, `PyPDFLoader` por exemplo, √© espec√≠fico para carregar um arquivo pdf, `WebBaseLoader` √© utilizado caso o objetivo √© carregar arquivos de uma p√°gina da web etc. Existem v√°rios outros componentes do `document_loader`, cada um espec√≠fico para diversas fontes de arquivos, [aqui](https://python.langchain.com/docs/integrations/document_loaders/) √© poss√≠vel encontrar todas as op√ß√µes.

Uma vez nossos arquivos carregados, √© realizado o processo de split destes arquivos, ou seja, o nosso objeto cont√©m uma lista de  objetos `Document`, onde cada um desses objetos cont√©m os textos retirados de nossos arquivos pdf, agora, esses textos ser√£o particionados em partes menores. Esse processo facilita na etapa de embedding e indexa√ß√£o.

```python
# Importando as depend√™ncias:
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Instanciando e estabelecendo o n√∫mero de characteres que cada parte de texto ter√°:
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)

# Particionando:
all_splits = text_splitter.split_document(documents=docs)
```
Agora com os arquivos devidamente repartidos em partes menores, j√° estamos prontos para o pr√≥ximo passo, embeddings e indexing.

O segundo passo define todo o processo de vetoriza√ß√£o e indexa√ß√£o dos arquivos previamente carregados e divididos no primeiro passo. Nessa etapa √© utilizado um modelo de llm para transformar cada peda√ßo de texto em embedding, um processo de atribui√ß√£o de um valor num√©rico para cada peda√ßo de texto. Posteriormente, ocorre a indexa√ß√£o desses embeddings, ou seja, o armazenamento em banco de dados local (Nesse caso, foi utilizado o Chroma).
![](/image/indexing_embedding.png)

```python
# Depend√™ncias:
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma

# Intanciando o modelo de embedding que vai gerar os vetores:
embeddings = GoogleGenerativeAIEmbeddings(model="Modelo do Seu LLM Aqui")

# Inicialiazando o banco de dados com o cliente Chroma:

# Diret√≥rio onde ser√° inicializado o banco com os vetores:
diretorio_chromadb = "../vector_store"

vector_store = Chroma(collection_name="Nome da cole√ß√£o", embedding_function=embeddings, persist_directory=diretorio_chromadb)

ids = vector_store.add_document(all_splits)  # Esta etapa indexa os vetores.
```

Na etapa exemplificada acima √© descrevido como √© feito o processo de instanciar o modelo de LLM escolhido, Google GenAI, ChatGpt, Claude etc, eles ser√°o os respons√°veis por gerar os vetores para posterior indexa√ß√£o pelo Chroma. Em seguida, iniciamos o banco de dados com o Chroma no diret√≥rio indicado, pois nesse caso estamos armazenando os vetores/embeddings localmente, mas √© perfeitamente poss√≠vel utilizar outros clientes, como por exemplo, AstraDB, FAISS, MongoDB etc, a lista com todas as op√ß√µes √© encontrada [aqui.](https://python.langchain.com/docs/integrations/vectorstores/) J√° sobre o modelos de embeddings dispon√≠veis, podem ser econtrados neste [link.](https://python.langchain.com/docs/integrations/text_embedding/)

O terceiro passo define o processo de busca com base nas perguntas feitas pelo usu√°rio. √â realizada uma recupera√ß√£o dos arquivos indexados no diret√≥rio escolhido no passo 2, e ent√£o esse resultado, chamado de contexto √© passado para o quarto passo para gerar a resposta final ao usu√°rio atrav√©s de um modelo de linguagem.
![](/image/retieve_generation.png)

```python
# Exemplo input:
perunta = "Qual o objetivo do projeto?"
resposta = vector_store.similarity_search(pergunta)
print(f"Resposta: {resposta}")
```

```python
# Output:
Resposta: O projeto tem o intuito apenas de demonstrar o passo a passo de uma simples aplica√ß√£o RAG (Retrieval-Augmented Generation). S√£o utilizados os components dos frameworks LangChain, langGraph e alguns components integrados como Google GenAI para os passos de vetoriza√ß√£o e genera√ß√£o do output, assim como tamb√©m o Chroma client para indexi√ß√£o dos embeddings localmente (no caso deste projeto) e posteriormente o passo de recupera√ß√£o.
```

Tanto a pergunta quanto a resposta s√£o passadas para o LLM para gerar o output ao usu√°rio, este passo √© definido no passo seguinte. 

O quarto passo √© onde definimos o prompt que ser√° passada ao llm, onde estar√° tanto a pergunta do usu√°rio quanto o contexto (o conte√∫do recuperado dos nossos arquivos) para que seja gerado o output.

```python
from langchain.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI

# Definimos o template:
template = """
Your are an AI assistent for answering questions based on the provide context.
Use the following pieces of retrieved context to answer the question.
If you don't know the answer, just say you don't know, don't try to make up an  answer.
Keep the answer concise an to the point.

Context: {context}

Question: {question}

Answer:
"""

# Instanciamos o template e o LLM:
rag_template = ChatPromptTemplate.from_template(template)
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)
```

```python
# Definimos o prompt:
message = rag_template.invoke({"question": pergunta, "context": resposta})
response = llm.invoke(message)
print(f"Resposta: {response.content}")
```

O quinto e √∫ltimo passo √© onde agrupamos todos os passos anteriores da nossa aplica√ß√£o com o LangGraph. Nesta etapa, definimos a sequ√™ncia de qual passo ser√° realizado primero e onde finalizar.

---

# Strutura do projeto

```text
.
‚îú‚îÄ‚îÄ üì¶ simple-rag-application/
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ docs                           # Arquivos pdf
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ vis√£o_do_projeto.pdf
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ image                          # Fluxo dos passos da aplica√ß√£o e demo
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_processing.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ indexing_embedding.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag_app_video.gif
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ retieve_generation.png
‚îÇ   ‚îî‚îÄ‚îÄ üìÇ script                         # C√≥digo fonte da aplica√ß√£o e arquivo .env
‚îÇ       ‚îú‚îÄ‚îÄ üêçüìÑ rag_app.py
‚îÇ       ‚îú‚îÄ‚îÄ .env                          # Arquivo .env (Opcional)
‚îÇ       ‚îî‚îÄ‚îÄ vector_store                  # Armazenamento dos vetores.
‚îú‚îÄ‚îÄ License                               # Licen√ßa MIT.
‚îî‚îÄ‚îÄ README.md                             # Vis√£o geral do projeto.
```

---

# Demo

Neste caso demostrado abaixo, a aplica√ß√£o foi inicializada no pr√≥rpio terminal com o comendo:
```python
python rag_app.py
```
![](/image/rag_app_video.gif)
